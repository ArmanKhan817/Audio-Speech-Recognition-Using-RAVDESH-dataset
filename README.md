# Audio-Speech-Recognition-Using-RAVDESH-dataset

The purpose of the communication is to exchange information, inform people about the incidents, influence people to do work, express ideas, etc. Emotion plays an imperative role during communication. It helps people to build trust, show empathy, influence, and win in negotiation. When a person communicates with another person, emotion gives insight into what action she or he should take. Even though it is easy for a human to understand the emotions like sad, happy, neutral, angry etc., it is challenging for the machine to extract emotion (El Ayadi et al., 2011, Schuller et al., 2011). AI (Artificial Intelligence) has made great progress in speech recognition, text to speech, speech to text, and sentiment analysis. It is still far off to understand emotions and interact with humans (Han et al., 2014, Ling et al., 2015). Speech emotion recognition could be one of the steps for the machine to understand our emotion and react. The application of this would-be automatic identification of customer satisfaction in a call centre, an ambient system that reacts based on mood, solves various language ambiguities, etc. Furthermore, speech emotion recognition is one of the focal research areas in the past few years (Tripathi et al., 2011). Therefore, this paper aims to explore machine learning model that detects emotion from the speech. Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) dataset will be used in this project and main is to identify emotion from the audio which has higher value based on MFCC (Livingstone and Russo, 2018, Logan, 2000).

![image](https://github.com/user-attachments/assets/4f2163de-09c3-4333-9605-3e3ac95cad74)

Figure 1: Block diagram of speech emotion recognition
